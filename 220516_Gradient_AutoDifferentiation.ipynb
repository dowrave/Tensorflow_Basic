{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220516_Gradient_AutoDifferentiation.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyO5o/+Fp9eRCneq04sEvhR+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dowrave/Tensorflow_Basic/blob/main/220516_Gradient_AutoDifferentiation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7WPXQmvkmpxP"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래디언트 계산하기\n",
        "- 자동 미분을 위해 tf는 정방향 패스 동안 연산의 순서를 기억하고, 역방향 패스 동안 이 목록을 역순으로 이동하여 그래디언트를 계산한다.\n",
        "- 텐서플로우는 이를 위한 `tf.GradientTape` API를 제공한다. 이는 컨텍스트 안에서 실행된 모든 연산을 테이프에 기록한 뒤 후진 방식 자동 미분(Reverse Mode Differentiation)을 통해 기록된 연산의 그래디언트를 계산한다."
      ],
      "metadata": {
        "id": "3K2L0RbVm5FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "with tf.GradientTape() as tape:\n",
        "  b = a**2 # 여기서 a와 b가 기록되는 것 같고\n",
        "\n",
        "db_da = tape.gradient(b, a) # 여기서 기록된 값들로 gradient를 구하는 것 같다.\n",
        "```"
      ],
      "metadata": {
        "id": "F4DI_n71pGWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x**2\n",
        "\n",
        "dy_dx = tape.gradient(y, x) # gradient(a, b) = da / db -> 즉 이 연산은 2 * 3 = 6\n",
        "dy_dx.numpy()"
      ],
      "metadata": {
        "id": "ZnsiVQbim3HO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 스칼라 외에도 모든 텐서에 대해 작동한다."
      ],
      "metadata": {
        "id": "6iV5pHdmnc8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "w = tf.Variable(tf.random.normal((3, 2)), name = 'w')\n",
        "b = tf.Variable(tf.zeros(2, dtype = tf.float32), name = 'b')\n",
        "x = [[1., 2., 3.]]\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  y = x @ w + b # @ : matmul\n",
        "  loss = tf.reduce_mean(y**2) # y**2 값 전체를 1개의 평균으로 냄 : 일단 변수로 갖고 있는 듯\n",
        "\n",
        "[dl_dw, dl_db] = tape.gradient(loss, [w, b]) "
      ],
      "metadata": {
        "id": "nZU1ND9knMoC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(loss) \n",
        "print(w.shape)\n",
        "print(dl_dw.shape)"
      ],
      "metadata": {
        "id": "HRtNi8GwoTYR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(dl_dw, dl_db)"
      ],
      "metadata": {
        "id": "loJG3sqpoU-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "my_vars = {\n",
        "    'w' : w,\n",
        "    'b' : b\n",
        "}\n",
        "\n",
        "grad = tape.gradient(loss, my_vars) # 위와 동일한데 그 형태만 dict로 전달한 거임\n",
        "grad['b']"
      ],
      "metadata": {
        "id": "71gfLMTXoZ0V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 모델에 대한 그래디언트\n",
        "- `tf.Varaibles`를 `tf.Module` 또는 해당 서브클래스 `layers.Layer` or `keras.Model` 중 하나로 수집한다.\n",
        "- `tf.Module`의 모든 서브클래스는 `Module.trainable_variables`로 변수를 집계하므로 그래디언트를 쉽게 계산할 수 있다."
      ],
      "metadata": {
        "id": "5ODnIUnEp7Dd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.Dense(2, activation = 'relu')\n",
        "x = tf.constant([[1., 2., 3.]])\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  # 정방향 연산 (forward pass)\n",
        "  y = layer(x)\n",
        "  loss = tf.reduce_mean(y**2)\n",
        "\n",
        "# 그래디언트 계산 - trainable_variables로 편안하게 모든 가중치와 바이어스를 지정했다.\n",
        "grad = tape.gradient(loss, layer.trainable_variables)\n",
        "\n",
        "for var, g in zip(layer.trainable_variables, grad):\n",
        "  print(f\"{var.name}, shape : {g.shape}\")"
      ],
      "metadata": {
        "id": "kNsYo5KjopFq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 테이프의 감시 대상 제어\n",
        "- 기본 : `tf.variable`에 액세스 후 모든 연산 기록하기\n",
        "- 왜?\n",
        "1. 테이프는 정방향 패스에 기록할 연산을 알아야 함\n",
        "2. 테이프는 중간 출력에 대한 참조를 보유함 : 불필요한 연산을 기록하지 않음\n",
        "3. 가장 일반적인 용례는 모든 모델의 훈련 가능한 변수에 대한 손실의 그래디언트 계산임"
      ],
      "metadata": {
        "id": "AVgBO_Jfq-rH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 훈련 가능\n",
        "x0 = tf.Variable(3., name = 'x0')\n",
        "\n",
        "# 훈련 불가능\n",
        "x1 = tf.Variable(3., name = 'x1', trainable = False)\n",
        "\n",
        "# Variable이 아님 (Varaible + Tensor = Tensor)\n",
        "x2 = tf.Variable(2., name = 'x2') + 1.0\n",
        "\n",
        "# Variable이 아님\n",
        "x3 = tf.constant(3., name = 'x3')\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = (x0**2) + (x1 ** 2) + (x2 ** 2)\n",
        "\n",
        "grad = tape.gradient(y, [x0, x1, x2, x3])\n",
        "\n",
        "for g in grad:\n",
        "  print(g) # Variable인 x0에 대한 그래디언트만 계산되었고, 나머지(trainable = False, Tensor, Constant(=Tensor)에 대한 값은 계산되지 않아 None 출력)"
      ],
      "metadata": {
        "id": "cT3JrgmTqZvQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 테이프에서 감시 중인 변수\n",
        "[var.name for var in tape.watched_variables()]"
      ],
      "metadata": {
        "id": "Y_wYvGf2rksW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.) # Tensor\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x) # Tensor에 대한 그래디언트를 기록하려면 watch 메소드가 필요하다.\n",
        "  y = x**2\n",
        "\n",
        "dy_dx = tape.gradient(y, x)\n",
        "print(dy_dx.numpy())"
      ],
      "metadata": {
        "id": "D6dlERpArzxy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = tf.Variable(0.)\n",
        "x1 = tf.Variable(10.)\n",
        "\n",
        "# 모든 Variable에 대한 기본 감시 비활성화 (watch_accessed_variables = False)\n",
        "with tf.GradientTape(watch_accessed_variables = False) as tape:\n",
        "  tape.watch(x1) # watch를 켜면 위에서 비활성화했어도 활성화됨\n",
        "  y0 = tf.math.sin(x0)\n",
        "  y1 = tf.nn.softplus(x1)\n",
        "  y = y0 + y1\n",
        "  ys = tf.reduce_sum(y)"
      ],
      "metadata": {
        "id": "sk8KXYeAr73C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "grad = tape.gradient(ys, {'x0' : x0, 'x1' : x1})\n",
        "print(grad['x0'], grad['x1']) # None, Tensor"
      ],
      "metadata": {
        "id": "udnhmbixsXrK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 중간 결과"
      ],
      "metadata": {
        "id": "-eh3-D4tsrzb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "tape.gradient(z, y).numpy() # dz / dy = 2y = 2x**2 = 18"
      ],
      "metadata": {
        "id": "5hg-eb1nscYG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GradientTape().gradient\n",
        "- 호출되면 GradientTape의 리소스가 해제된다.\n",
        "- 같은 계산에 대해 여러 그래디언트를 계산하려면 `persistent = True`인 그래디언트 테이프를 만든다.\n",
        "- 계산 후에는 `del tape` 해주면 됨"
      ],
      "metadata": {
        "id": "BcD0Cb_MtCVO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant([1, 3.]) # 이 경우 type은 float32가 됨\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  tape.watch(x)\n",
        "  y = x * x\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x).numpy()) # dz_dx = 4x**3 = 3, 108\n",
        "print(tape.gradient(y, x).numpy()) # dy_dx = 2x = 2, 6"
      ],
      "metadata": {
        "id": "wTGJkHH3s-hw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 성능 참고 사항\n",
        "- 테이프 컨텍스트 내에서 연산 수행하는 것에 대해 작은 오버헤드가 있다. 필요한 경우에만 `tape`를 사용할 것\n",
        "- 그래디언트 테이프는 입출력을 포함한 중간 결과를 저장한다. 효율성을 위해 일부 연산(`ReLU` 등)은 중간 결과를 유지할 필요가 없고, 이는 정방향 패스 동안 정리된다."
      ],
      "metadata": {
        "id": "sp-2AbDgtrAi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 그래디언트는 기본적으로 스칼라에 대한 연산이다.\n",
        "- 여러 대상의 그래디언트를 요청하면? - 각 대상의 그래디언트의 합계가 나옴"
      ],
      "metadata": {
        "id": "r24ckCytt-vB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "with tf.GradientTape() as tape:\n",
        "  y0 = x**2\n",
        "  y1 = 1 / x\n",
        "\n",
        "print(tape.gradient({'y0' : y0, 'y1' : y1}, x).numpy()) # dy0_dx1, dy1_dx 가 아니라 이 둘의 합계가 나옴"
      ],
      "metadata": {
        "id": "f3g5Rk_UtmII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  y = x * [3., 4.]\n",
        "\n",
        "print(tape.gradient(y, x).numpy()) # 마찬가지로 그래디언트 값이 행렬이지만, 결과는 각 원소의 합계가 나온다."
      ],
      "metadata": {
        "id": "NjaVuQaNuUkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 별도의 그래디언트가 필요하다면 Jakovian을 참고하라고 함. 나중에 고급 AutoDiff에서 나옴"
      ],
      "metadata": {
        "id": "0IMq0Xhiu2NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.linspace(-10., 10., 200 + 1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  tape.watch(x)\n",
        "  y = tf.nn.sigmoid(x) # 시그모이드 자체\n",
        "\n",
        "dy_dx = tape.gradient(y, x) # 시그모이드를 다시 x로 미분한 값\n",
        "\n",
        "plt.plot(x, y, label = 'y')\n",
        "plt.plot(x, dy_dx, label = 'dy_dx')\n",
        "plt.legend()\n",
        "\n",
        "_ = plt.xlabel('x')"
      ],
      "metadata": {
        "id": "KjCOuE0lutYe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 흐름 제어하기\n",
        "-  Python의 문법이 자연스럽게 처리됨"
      ],
      "metadata": {
        "id": "Aq35RL5lvZ1e"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(1.)\n",
        "\n",
        "v0 = tf.Variable(x)\n",
        "v1 = tf.Variable(2.)\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  tape.watch(x)\n",
        "\n",
        "  # 이 경우 result 값은 if 조건이 True인 곳에 대해서만 값이 저장된다.\n",
        "  if x > 0.:\n",
        "    result = v0\n",
        "  else:\n",
        "    result = v1 ** 2\n",
        "\n",
        "dv0, dv1 = tape.gradient(result, [v0, v1])\n",
        "\n",
        "print(dv0, dv1)"
      ],
      "metadata": {
        "id": "xzwcMYM4vPuW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dx = tape.gradient(result, x)\n",
        "print(dx) # x값에 따라 result는 v0나 v1**2이다. 그러나 result는 x에 관한 식이 아님 -> None이 나올 수 밖에"
      ],
      "metadata": {
        "id": "6iT3C_FHvoex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### None의 Gradient 구하기"
      ],
      "metadata": {
        "id": "6rNx89Fhxkix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape(persistent = True) as tape:\n",
        "  z = y * y\n",
        "\n",
        "print(tape.gradient(z, x)) # x는 tape에 기록되지 않음\n",
        "\n",
        "# persistent = True가 없다면 아래 코드는 실행되지 않음 : 위의 gradient 식으로 인해 tape에 기록된 값들이 날아갔기 때문에\n",
        "print(tape.gradient(z, y))"
      ],
      "metadata": {
        "id": "r0ZLIcVMwF-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 그래디언트의 연결이 끊어지는 사유들이 있다."
      ],
      "metadata": {
        "id": "sOvcLN2Fx-B_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. `Variables.assign_add()` 대신 `Variables` 객체에 Operator를 가해서 `Tensor`로 바뀌는 경우"
      ],
      "metadata": {
        "id": "7oZmeVXo0m9q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable(2.)\n",
        "\n",
        "for epoch in range(2):\n",
        "  with tf.GradientTape() as tape:\n",
        "    y = x + 1\n",
        "\n",
        "  print(type(x).__name__, \" : \", tape.gradient(y, x))\n",
        "  # x = x + 1 # Variable에 스칼라 연산을 가하면 Tensor가 됨 - 그래디언트가 끊김\n",
        "  x.assign_add(1) # Variable 업데이트는 해당 메소드를 이용한다.\n"
      ],
      "metadata": {
        "id": "1RPNM9HnxMW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. `Tensorflow` 외부에서 계산된 경우"
      ],
      "metadata": {
        "id": "Pl2lzo7S0Uhv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([[1., 2.],\n",
        "                 [3., 4.]], dtype = tf.float32)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x2 = x**2\n",
        "\n",
        "  # Tensorflow가 아닌 Numpy로 계산되었음에 주목\n",
        "  y = np.mean(x2, axis=0)\n",
        "\n",
        "  # y는 array이기 때문에 텐서플로우의 연산을 적용할 수 없다.\n",
        "  y = tf.reduce_mean(y, axis = 0)\n",
        "\n",
        "  # 아래처럼 텐서로 바꾼 뒤 연산을 적용할 것\n",
        "  # y = tf.convert_to_tensor(y)\n",
        "  # y = tf.reduce_mean(x2, axis = 0)\n",
        "\n",
        "print(tape.gradient(y, x))"
      ],
      "metadata": {
        "id": "agmkHxjsyLYw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. int, string을 통해 gradient를 구한 경우\n",
        "- 그래디언트 계산은 `float`만 쓰인다는 것만 짚어두자"
      ],
      "metadata": {
        "id": "XLEQszKA1iMN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.constant(10)\n",
        "\n",
        "with tf.GradientTape() as g:\n",
        "  g.watch(x)\n",
        "  y = x * x\n",
        "\n",
        "print(g.gradient(y, x)) # 그래디언트 누락 대신 유형 오류가 발생함"
      ],
      "metadata": {
        "id": "Mx1O-IPI1Njm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. 상태 저장 개체로 그래디언트를 구함\n",
        "- 테이프는 현재 상태만 볼 수 있다. 현재 상태에 이르게 된 기록은 볼 수 없다.\n",
        "- `tf.Tensor` : 텐서가 작성된 후에는 변경할 수 없다. 값은 있지만 상태는 없다. 여태까지의 모든 연산은 상태 비저장임\n",
        "- `tf.Variable` : 내부 상태 & 값을 갖는다. 변수와 관련한 그래디언트를 계산하는 게 일반적이나, 변수의 상태는 그래디언트 계산이 더 멀리 돌아가지 않도록 차단한다."
      ],
      "metadata": {
        "id": "HoH9T8rL12Av"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x0 = tf.Variable(3.)\n",
        "x1 = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  x1.assign_add(x0) # x1에 x1 + x0이 할당됨. 별도의 = 필요 없나봄?\n",
        "\n",
        "  # y는 (x1 + x0) ** 2 부터 기록이 시작됨 - x1부터 기록되지 않음\n",
        "  y = x1 ** 2\n",
        "\n",
        "print(tape.gradient(y, x0)) # 예상 : dy_dx0 = 2(x1 + x0)\n",
        "                            # 실제 : dy_dx0 = None"
      ],
      "metadata": {
        "id": "6N-Oonho1pn7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 그래디언트가 등록되지 않는 경우\n",
        "- 일부 `tf.Operation`은 미분 불가능한 것으로 등록되어 `None`을 반환함.\n",
        "- `tf.raw_ops` 페이지에는 그래디언트가 등록된 저수준 연산이 표시됨\n",
        "- 그래디언트가 등록되지 않은 연산으로 미분하고 싶다면, 그래디언트를 구현하고 등록(`tf.RegisterGradient`)하거나 다른 ops로 함수를 다시 구현해야 함"
      ],
      "metadata": {
        "id": "xjMcEsK621Ev"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "image = tf.Variable([[[.5, .0, .0]]])\n",
        "delta = tf.Variable(.1)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  new_image = tf.image.adjust_contrast(image, delta) #그래디언트를 가질 수 있음\n",
        "\n",
        "try:\n",
        "  print(tape.gradient(new_image, [image, delta])) # 구현되지 않은 raw_ops.AdjustContrastV2를 반환함\n",
        "  assert False \n",
        "except LookupError as e: # 그래디언트가 등록되지 않은 float op -> 오류 발생시킴\n",
        "  print(f\"{type(e).__name__} : {e}\")"
      ],
      "metadata": {
        "id": "wnV7BfuS2OFb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 연결되지 않은 그래디언트 : None 대신 0"
      ],
      "metadata": {
        "id": "hXsbuzq53y6q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x = tf.Variable([2., 2.])\n",
        "y = tf.Variable(3.)\n",
        "\n",
        "with tf.GradientTape() as tape:\n",
        "  z = y**2\n",
        "\n",
        "print(tape.gradient(z, x, unconnected_gradients = tf.UnconnectedGradients.ZERO)) # dz_dx는 None이 떠야 하지만.."
      ],
      "metadata": {
        "id": "zRqvxeRO3OiD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "B6rRS5o-39iQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}