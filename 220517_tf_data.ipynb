{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220517_tf_data.ipynb",
      "private_outputs": true,
      "provenance": [],
      "authorship_tag": "ABX9TyNSGUi38MMhuLBBZBnVF1Sp",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dowrave/Tensorflow_Basic/blob/main/220517_tf_data.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# tf.data\n",
        "- `tf.data` API로 간단하고 재사용 가능한 조각으로 입력 파이프라인을 빌드할 수 있다.\n",
        "  - 예를 들어 이미지 모델의 파이프라인은 분산된 파일 시스템의 파일에서 데이터 집계, 각 이미지에 임의의 Perturbation 적용, 무작위로 선택한 이미지를 학습을 위한 batch로 병합할 수 있음.\n",
        "  - 텍스트 모델의 경우 원시 텍스트 데이터에서 심볼 추출, 룩업 테이블이 있는 embedding 식별자로 변환, 길이가 서로 다른 시퀀스를 batch 처리하는 과정을 포함할 수 있다.\n",
        "\n",
        "- `tf.data` API는 `tf.data.Dataset` 추상화를 도입하며, 각 요소는 하나 이상의 구성요소로 이루어진다. 예를 들어 이미지라면 (이미지, 해당 label)인 것.\n",
        "\n",
        "- 데이터 세트 생성 방법 \n",
        "1. 데이터 소스 : 메모리 또는 하나 이상의 파일에 저장된 데이터에서 데이터셋 구성\n",
        "2. 데이터 변환 : `tf.data.Dataset` 객체에서 데이터셋 구성"
      ],
      "metadata": {
        "id": "VHgNfHKW4C-W"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8oY6fJB14AXV"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import pathlib\n",
        "import os\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "np.set_printoptions(precision = 4) # 소수점 4번째 자리까지?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 기본 매커니즘\n",
        "- `데이터 소스`로 시작 \n",
        "- 메모리의 데이터에서 `Dataset`를 구성하겠다면 `tf.data.Dataset.from_tensors()` 또는 `tf.data.Dataset.from_tensor_slices()`을 쓸 수 있다. \n",
        "- TFRecord로 저장되었다면 `tf.data.TFRecordDataset()`을 쓸 수도 있다.\n",
        "\n",
        "- `Dataset` 객체는 메서드 호출로 새로운 `Dataset`로 변환할 수 있다. \n",
        "  - 요소별 변환 : `Dataset.map()`\n",
        "  - 다중 요소 변환 : `Dataset.batch()`\n",
        "\n",
        "- 파이썬 반복문도 돌릴 수 있음"
      ],
      "metadata": {
        "id": "RgT-QeJo4-Fa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])\n",
        "dataset"
      ],
      "metadata": {
        "id": "lyYE_h5n48-a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for elem in dataset:\n",
        "  print(elem.numpy())"
      ],
      "metadata": {
        "id": "RV41BFm05dVl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "it = iter(dataset) # 파이썬 반복기 생성해서\n",
        "\n",
        "# 1개씩 뽑을 수도 있음\n",
        "print(next(it).numpy()) \n",
        "print(next(it).numpy()) "
      ],
      "metadata": {
        "id": "9o7om0lp5fei"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# reduce를 사용해 데이터세트 요소들을 소비할 수도 있다.\n",
        "print(dataset.reduce(0, lambda state, value : state + value).numpy())"
      ],
      "metadata": {
        "id": "rL9fzjTE5kqr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터세트 구조\n",
        "- 데이터세트는 각 요소가 동일한 구성 요소(중첩) 구조를 갖는 요소의 시퀀스를 생성한다.\n",
        "- 개별 구성요소는 `tf.TypeSpec`에 표현되는 유형일 수 있는데, `tf.Tensor`, `tf.TensorArray`, `tf.data.Dataset` 등도 포함된다.\n",
        "<br>\n",
        "- 파이썬 구조에는 `tuple`, `dict`, `NamedTuple`, `OrderedDict`가 있다.\n",
        "- `list`는 데이터세트 요소의 구조를 표현하는 데 유효하지 않다. `list`는 `tuple`로 변환해야 구조로 처리할 수 있다. `list` 출력을 단일 구성 요소로 쓰고 싶다면 `tf.stack`을 사용해 이를 명시적으로 구성해야 한다.\n",
        "<br>\n",
        "- `Dataset.element_spec` 속성으로 각 요소의 구성 요소 유형을 검사할 수 있다."
      ],
      "metadata": {
        "id": "cV_R16Tq5zsH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(tf.random.uniform([4, 10]))\n",
        "dataset1.element_spec"
      ],
      "metadata": {
        "id": "w8Qwl43n5ycK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([4]),\n",
        "     tf.random.uniform([4, 100], maxval = 100, dtype =tf.int32))\n",
        ")\n",
        "\n",
        "dataset2.element_spec"
      ],
      "metadata": {
        "id": "Ds4gy37h6b-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# zip 함수로 묶을 수 있음\n",
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "\n",
        "dataset3.element_spec"
      ],
      "metadata": {
        "id": "nke9Y0nw6j_S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# sparse 텐서도 포함 가능\n",
        "dataset4 = tf.data.Dataset.from_tensors(tf.SparseTensor(indices = [[0, 0], [1, 2]], values = [1, 2], dense_shape = [3,4]))\n",
        "\n",
        "dataset4.element_spec"
      ],
      "metadata": {
        "id": "0Ltbr2hH60cp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# value_type으로 대표되는 데이터 타입을 볼 수도 있다.\n",
        "dataset4.element_spec.value_type"
      ],
      "metadata": {
        "id": "oagg4NQo7EVb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dataset` 변환은 모든 구조의 데이터세트를 지원한다."
      ],
      "metadata": {
        "id": "mziZ0LwC7T9r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset1 = tf.data.Dataset.from_tensor_slices(\n",
        "    tf.random.uniform([4, 10], minval = 1, maxval = 10, dtype = tf.int32)\n",
        ")\n",
        "dataset1"
      ],
      "metadata": {
        "id": "cg5B-cYu7RWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset2 = tf.data.Dataset.from_tensor_slices(\n",
        "    (tf.random.uniform([4]),\n",
        "     tf.random.uniform([4, 100], maxval = 100, dtype = tf.int32))\n",
        ")"
      ],
      "metadata": {
        "id": "jZKQK-M67dTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset3 = tf.data.Dataset.zip((dataset1, dataset2))\n",
        "dataset3"
      ],
      "metadata": {
        "id": "LNxlsg5y7t0N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for a, (b, c) in dataset3:\n",
        "  print(a.shape, b.shape, c.shape)"
      ],
      "metadata": {
        "id": "NUEUXKdI7wSU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 입력 데이터 읽기\n",
        "### Numpy Array\n",
        "- 모든 입력 데이터가 메모리에 맞다면 `Tensor` 객체로 변환 후 `Dataset.from_tensor_slices()`를 이용한다.\n"
      ],
      "metadata": {
        "id": "ekWjslK975XZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()"
      ],
      "metadata": {
        "id": "790gWhR370Gm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "images, labels = train\n",
        "images = images/255.\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ],
      "metadata": {
        "id": "yODKjUgr7_Rn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위 방법은 `feature, label` 배열을 `tf.constant()` 연산으로 tf 그래프에 임베딩 처리한다. 배열의 내용이 여러 번 복사되기 때문에 `tf.GraphDef` 프로토콜 버퍼의 2GB 제한에 걸릴 수 있음"
      ],
      "metadata": {
        "id": "eDOQUmL28Z8H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Python Generator\n",
        "- 편리하지만 이식성, 확장성이 제한적임.\n",
        "- 생성기를 생성한 것과 동일한 파이썬 프로세스에서 실행되어야 하며 여전히 파이썬 GIL의 영향을 받는다.\n"
      ],
      "metadata": {
        "id": "KZmn4-Wd8j6d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def count(stop):\n",
        "  i = 0\n",
        "  while i < stop:\n",
        "    yield i # yield : generator object를 반환함\n",
        "            # generator : 데이터를 미리 만들지 않고 필요할 때 하나씩 만들어내는 객체\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "DGpEZIWm8FzA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 잠깐! yield와 return 차이에 대해서 짚고 넘어감\n",
        "[차이점]\n",
        "<br>\n",
        "1. 아래 함수에서 return은 리스트를, yield는 generator를 반환함.\n",
        "2. `return`은 모든 결과값을 메모리에 올려야 하지만, `yield`는 결과값을 하나씩 메모리에 올려놓음 - `lazy iterator`라고도 함. 메모리 관리 측면에서 yield가 더 효율적임\n",
        "  - 이런 특성이 있기 때문에 데이터를 무한히 뱉는 함수는 `yield`가 꼭 필요함(무한 while문에서 return을 잡으면 그게 다 메모리로 잡히지만, yield로 잡으면 1개씩 뱉으니까)\n",
        "\n",
        "[리스트 -> 제너레이터 변환]\n",
        "<br>\n",
        "- return 대신 yield from 쓰면 됨\n",
        "\n",
        "[generator comprehension]\n",
        "<br>\n",
        "- [] 대신 ()쓰면 됨. 사용법은 동일"
      ],
      "metadata": {
        "id": "cxj61ka_9LXJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def return_abc():\n",
        "  alphabets = []\n",
        "  for ch in \"ABC\":\n",
        "    time.sleep(1)\n",
        "    alphabets.append(ch)\n",
        "  return alphabets\n",
        "\n",
        "def yield_abc():\n",
        "  for ch in \"ABC\":\n",
        "    time.sleep(1)\n",
        "    yield ch\n",
        "\n",
        "# 3초 후 abc가 한꺼번에 출력됨\n",
        "for ch in return_abc():\n",
        "  print(ch) \n",
        "\n",
        "# 1초 후 a, 1초 후 b, 1초 후 c가 출력됨.\n",
        "for ch in yield_abc():\n",
        "  print(ch)"
      ],
      "metadata": {
        "id": "Al4ZLej49K4f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for n in count(5):\n",
        "  print(n)"
      ],
      "metadata": {
        "id": "6TkgD8ss9bJs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 이렇게 파이썬으로 생성된 생성자를 `Dataset.from_generator`로 받을 수 있음.\n",
        "- 이 생성자는 callable을 입력으로 받으며, 생성기가 끝에 도달하면 다시 시작할 수 있다."
      ],
      "metadata": {
        "id": "WurxUyI8AK6E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_counter = tf.data.Dataset.from_generator(count, args = [25], output_types = tf.int32, output_shapes = (),)\n",
        "for count_batch in ds_counter.repeat().batch(10).take(10):\n",
        "  print(count_batch.numpy())"
      ],
      "metadata": {
        "id": "_JJjz7Q7--7g"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`output_shapes` : 많은 텐서플로우 연산이 모르는 랭크의 텐서를 지원하지 않기 때문에 쓰는 걸 권장함. 축의 길이를 모르거나 가변적이라면 해당 값을 `None`으로 설정할 것."
      ],
      "metadata": {
        "id": "hJjGCbBuAhix"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def gen_series():\n",
        "  i = 0\n",
        "  while True:\n",
        "    size = np.random.randint(0, 10)\n",
        "    yield i, np.random.normal(size = (size, ))\n",
        "    i += 1"
      ],
      "metadata": {
        "id": "3XEgpENmAdnY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i, series in gen_series():\n",
        "  print(i, \" : \", str(series))\n",
        "  if i > 5:\n",
        "    break"
      ],
      "metadata": {
        "id": "LrYY0W4UAyhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds_series = tf.data.Dataset.from_generator(\n",
        "    gen_series,\n",
        "    output_types = (tf.int32, tf.float32),\n",
        "    output_shapes = ((), (None,)) # 2번째 항목이 알 수 없는 길이 None으로 전달됨\n",
        ")\n",
        "\n",
        "ds_series"
      ],
      "metadata": {
        "id": "SddDikMtA2Rh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가변 형상의 데이터세트 배치 처리는 `Dataset.padded_batch`를 사용한다."
      ],
      "metadata": {
        "id": "_7zU4qBvBWfI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds_series_batch = ds_series.shuffle(20).padded_batch(10) \n",
        "\n",
        "ids, sequence_batch = next(iter(ds_series_batch))\n",
        "print(ids.numpy())\n",
        "print()\n",
        "print(sequence_batch.numpy())"
      ],
      "metadata": {
        "id": "1P1UYHGwBKe-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 더 현실적인 예시\n",
        "`preprocessing.image.ImageDataGenerator`를 `tf.data.Dataset`으로 래핑하기"
      ],
      "metadata": {
        "id": "8pd0IGxFBrEM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flowers = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar = True\n",
        ")"
      ],
      "metadata": {
        "id": "kkVm8mbSBfkJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ImageDataGenerator 만들기\n",
        "img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale = 1. / 255, rotation_range = 20)\n",
        "\n",
        "images, labels = next(img_gen.flow_from_directory(flowers))"
      ],
      "metadata": {
        "id": "7skiVOZfB3K6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(images.dtype, images.shape)\n",
        "print(labels.dtype, labels.shape)"
      ],
      "metadata": {
        "id": "dDkezce4CB99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = tf.data.Dataset.from_generator(\n",
        "    lambda : img_gen.flow_from_directory(flowers), \n",
        "    output_types = (tf.float32, tf.float32), \n",
        "    output_shapes = ([32, 256, 256, 3], [32, 5])\n",
        "    )\n",
        "\n",
        "ds.element_spec"
      ],
      "metadata": {
        "id": "yBe3tGblCFyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for images, label in ds.take(1):\n",
        "  print(images.shape, labels.shape)"
      ],
      "metadata": {
        "id": "YrOK1fqfCQg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### TFRecord 데이터 소비하기\n",
        "- `tf.data` API는 메모리에 맞지 않는 큰 데이터세트를 처리할 수 있다.\n",
        "- `tf.data.TFRecordDataset` 클래스로 하나 이상의 TFRecord 파일 내용을 스트리밍할 수 있다."
      ],
      "metadata": {
        "id": "d5RBVsuoCnmc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsns_test_file = tf.keras.utils.get_file(\"fsns.tfrec\",\n",
        "                                         \"https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001\")\n"
      ],
      "metadata": {
        "id": "XqcV3FANCl2z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `TFRecordDataset`의 `filenames`는 문자열, 문자열 목록, `tf.Tensor` 문자열일 수 있음.\n",
        "- 학습, 검증 목적으로 두 파일 세트를 사용한다면 파일 이름을 입력 인수로 써서 데이터 세트를 생성하는 팩토리 메소드를 작성할 수 있다."
      ],
      "metadata": {
        "id": "b8rQyAxwDEZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ],
      "metadata": {
        "id": "sgZ7uQKjC7b1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "parsed.features.feature['image/text']"
      ],
      "metadata": {
        "id": "jI9PZCWKDSRI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 텍스트 데이터 소비하기"
      ],
      "metadata": {
        "id": "GAzRtSzGDgtP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "directory_url = 'https://storage.googleapis.com/download.tensorflow.org/data/illiad/'\n",
        "filenames = ['cowper.txt', 'derby.txt', 'butler.txt'] # 여러 파일을 가져올 수 있음\n",
        "\n",
        "file_paths = [\n",
        "              tf.keras.utils.get_file(file_name, directory_url + file_name) for file_name in filenames\n",
        "]"
      ],
      "metadata": {
        "id": "5h4WFHpBDZw5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TextLineDataset(file_paths)"
      ],
      "metadata": {
        "id": "ZTx7BSp5DxDd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in dataset.take(5):\n",
        "  print(line.numpy())"
      ],
      "metadata": {
        "id": "dZrPqOPKDz_-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 파일 사이에서 줄 바꾸기\n",
        "files_ds = tf.data.Dataset.from_tensor_slices(file_paths)\n",
        "lines_ds = files_ds.interleave(tf.data.TextLineDataset, cycle_length = 3) # 이걸로 파일을 쉽게 섞을 수 있다.\n",
        "\n",
        "for i, line in enumerate(lines_ds.take(9)):\n",
        "  if i % 3 == 0:\n",
        "    print()\n",
        "  print(line.numpy())"
      ],
      "metadata": {
        "id": "srH5ZuH8D1iT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `TextLineDataset`은 각 파일의 모든 줄을 생성한다 : 헤더 줄로 시작하거나 주석이 포함되었다면 바람직하지 않을 수 있음.\n",
        "  - 이러한 줄은 `Dataset.skip()` 또는 `Dataset.filter()` 변환으로 제거할 수 있다.\n"
      ],
      "metadata": {
        "id": "27DOGOPsEHCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_file = tf.keras.utils.get_file('train.csv', 'https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ],
      "metadata": {
        "id": "SYwy2efkEBzP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in titanic_lines.take(10):\n",
        "  print(line.numpy())"
      ],
      "metadata": {
        "id": "bkaNHoEMEfyP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def survived(line):\n",
        "  return tf.not_equal(tf.strings.substr(line, 0, 1), \"0\")\n",
        "\n",
        "survivors = titanic_lines.skip(1).filter(survived)"
      ],
      "metadata": {
        "id": "eog1OVmMEh3z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for line in survivors.take(10):\n",
        "  print(line.numpy())"
      ],
      "metadata": {
        "id": "JVOp7EutEpAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### CSV 데이터 소비하기"
      ],
      "metadata": {
        "id": "L4B0V2XOExTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_file = tf.keras.utils.get_file('train.csv', 'https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "df = pd.read_csv(titanic_file)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "W9IOEhW4EqsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dataset.from_tensor_slices` : 데이터가 메모리에 맞다면 사용가능"
      ],
      "metadata": {
        "id": "Q7ezgzXHE5lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_slices = tf.data.Dataset.from_tensor_slices(dict(df))\n",
        "\n",
        "for feature_batch in titanic_slices.take(1):\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"{!r:20s} : {}\".format(key, value))"
      ],
      "metadata": {
        "id": "8IMRQqFDE1-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 더 확장성 있는 방식은 필요에 따라 디스크에서 로드하는 것이다.\n",
        "- `tf.data`는 RFC 4180을 준수하는 하나 이상의 CSV 파일에서 레코드를 추출하는 메소드를 제공한다.\n",
        "- `experimental.make_csv_dataset` 함수는 csv 파일 세트를 읽는 고급 인터페이스로, 열 형식 유추를 비롯해 배치, 셔플 등 많은 기능을 지원함"
      ],
      "metadata": {
        "id": "b-vaZS50FOd3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_batches = tf.data.experimental.make_csv_dataset(titanic_file, batch_size = 4, label_name = 'survived')"
      ],
      "metadata": {
        "id": "uqi4H2mjFBTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"Survived : {}\".format(label_batch))\n",
        "  print(\"Features : \")\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\"{!r:20s}: {}\".format(key, value))\n"
      ],
      "metadata": {
        "id": "m8JMRU_TFfO6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 열의 일부만 필요하다면 select_columns 인수를 이용한다.\n",
        "titanic_batches = tf.data.experimental.make_csv_dataset(\n",
        "    titanic_file, batch_size = 4,\n",
        "    label_name = 'survived', select_columns = ['class', 'fare', 'survived']\n",
        ")\n",
        "\n",
        "for feature_batch, label_batch in titanic_batches.take(1):\n",
        "  print(\"'survived' : {}\".format(label_batch))\n",
        "  for key, value in feature_batch.items():\n",
        "    print(\" {!r:20s} : {}\".format(key, value))"
      ],
      "metadata": {
        "id": "gaohWvm_Frbi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 더 세밀한 제어를 지원하는 `experimental.CsvDataset` 클래스도 있다. 열 형식 유추를 지원하지 않고, 각 열의 유형을 지원해야 함. 귀찮으니 생략.\n",
        "- 헤더 제거 기능은 지원한다. `header` 혹은 `select_cols`가 있음."
      ],
      "metadata": {
        "id": "zPuiSE2jGEiY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 파일 세트 소비하기"
      ],
      "metadata": {
        "id": "oIx7HfJ7GXUI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "flowers_root = tf.keras.utils.get_file(\n",
        "    'flower_photos',\n",
        "    'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "    untar = True\n",
        ")\n",
        "flowers_root = pathlib.Path(flowers_root)\n",
        "\n",
        "for item in flowers_root.glob('*'):\n",
        "  print(item.name)"
      ],
      "metadata": {
        "id": "DcxOsWaUGALK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/'*/*'))\n",
        "\n",
        "for f in list_ds.take(5):\n",
        "  print(f.numpy())"
      ],
      "metadata": {
        "id": "DJgbIFUfGlQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def process_path(file_path):\n",
        "  label = tf.strings.split(file_path, os.sep)[-2]\n",
        "  return tf.io.read_file(file_path), label\n",
        "\n",
        "labeled_ds = list_ds.map(process_path)"
      ],
      "metadata": {
        "id": "g9nY6LP4GyXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for image_raw, label_text in labeled_ds.take(1):\n",
        "  print(repr(image_raw.numpy()[:100]))\n",
        "  print()\n",
        "  print(label_text.numpy())"
      ],
      "metadata": {
        "id": "gzcY2OFoG8RY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 요소 배치 처리"
      ],
      "metadata": {
        "id": "RdKA6rEQHGuL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 간단한 배치 처리\n",
        "- `Dataset.batch()`"
      ],
      "metadata": {
        "id": "A5yaXfRvHJrX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inc_dataset = tf.data.Dataset.range(100)\n",
        "dec_dataset = tf.data.Dataset.range(0, -100, -1)\n",
        "dataset = tf.data.Dataset.zip((inc_dataset, dec_dataset))\n",
        "batched_dataset = dataset.batch(4)\n",
        "\n",
        "for batch in batched_dataset.take(4):\n",
        "  print([arr.numpy() for arr in batch])"
      ],
      "metadata": {
        "id": "HjDa9JzhHFHf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_dataset # shape = None - 마지막 배치가 완전히 채워지지 않을 가능성이 있어서 None(동적 가능)으로 둠"
      ],
      "metadata": {
        "id": "Mx6g0T-2HcOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "batched_dataset = dataset.batch(7, drop_remainder = True) # 꽉 채워지지 않는 마지막 배치는 무시함\n",
        "batched_dataset # shape가 7로 채워진 것을 볼 수 있음"
      ],
      "metadata": {
        "id": "O57R32wrHnIk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 패딩이 있는 텐서 배치 처리\n",
        "- 많은 모델은 다양한 크기를 가지는 입력 데이터로 작동한다.\n",
        "- 이 때 `Dataset.padded_batch` 변환으로 패딩 처리될 수 있는 하나 이상의 차원을 지정해 서로 다른 형상의 텐서를 배치 처리할 수 있다."
      ],
      "metadata": {
        "id": "N74mXcuMH6-N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.range(100)\n",
        "dataset = dataset.map(lambda x : tf.fill([tf.cast(x, tf.int32)], x)) # [[], [1], [2, 2], [3, 3, 3]], ...\n",
        "dataset = dataset.padded_batch(4, padded_shapes = (None,)) # batch_size = 4로 쪼갬 / shape = None은 가변 길이를 의미함\n",
        "\n",
        "for batch in dataset.take(2):\n",
        "  print(batch.numpy())\n",
        "  print()"
      ],
      "metadata": {
        "id": "V2IJh4niH1B9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 학습 워크 플로우"
      ],
      "metadata": {
        "id": "npm_74djIsZq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 여러 에포크 처리하기\n",
        "- 데이터 세트를 반복하는 가장 간단한 방법은 `Dataset.repeat()` 변환을 사용하는 것이다."
      ],
      "metadata": {
        "id": "rabzWcivIuvu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_file = tf.keras.utils.get_file('train.csv', 'https://storage.googleapis.com/tf-datasets/titanic/train.csv')\n",
        "titanic_lines = tf.data.TextLineDataset(titanic_file)"
      ],
      "metadata": {
        "id": "yyQhJbQdIO5O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_batch_sizes(ds):\n",
        "  batch_sizes = [batch.shape[0] for batch in ds]\n",
        "  plt.bar(range(len(batch_sizes)), batch_sizes)\n",
        "  plt.xlabel('Batch Number')\n",
        "  plt.ylabel('Batch Size')"
      ],
      "metadata": {
        "id": "zg37mCQXI6lQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dataset.repeat` : 인수가 없다면 무한\n",
        "- 한 에포크의 끝과 다음 에포크의 시작을 알리지 않고 인수를 연결한다.\n",
        "- 즉 `Dataset.batch` 후의 `Dataset.repeat`는 에포크 경계 양쪽에 걸쳐진 배치가 생성된다."
      ],
      "metadata": {
        "id": "-S3ow8ecJQvW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_batches = titanic_lines.repeat(3).batch(128)\n",
        "plot_batch_sizes(titanic_batches)"
      ],
      "metadata": {
        "id": "Dpo16ED0JCoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 만약 에포크 분리가 필요하다면, 반복 전에 `Dataset.batch`를 넣자."
      ],
      "metadata": {
        "id": "3OJT_JubJcV5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "titanic_batches = titanic_lines.batch(128).repeat(3)\n",
        "\n",
        "plot_batch_sizes(titanic_batches)"
      ],
      "metadata": {
        "id": "qykSF3RSJN4N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 에포크의 끝에서 사용자 정의 계산(통계 수집 등)을 실행하려면 각 에포크에서 데이터 세트 반복을 다시 시작하는 게 가장 간단하다"
      ],
      "metadata": {
        "id": "SIrASWzsJk6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 3\n",
        "dataset = titanic_lines.batch(128)\n",
        "\n",
        "for epoch in range(epochs):\n",
        "  for batch in dataset:\n",
        "    print(batch.shape)\n",
        "\n",
        "  print(\"End of Epoch : \", epoch)"
      ],
      "metadata": {
        "id": "_m5NLecVJixZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 입력 데이터의 임의 셔플\n",
        "- `Dataset.shuffle()` 변환은 고정 크기 버퍼 유지 & 해당 버퍼에서 다음 요소 무작위로 균일하게 선택함\n",
        "- buffer_sizes를 크게 가져간다면 더 철저하게 셔플되나 더 많은 메모리와 시간이 걸릴 수 있다. 문제가 된다면 파일 전체에서 `Dataset.interleave`를 사용하자."
      ],
      "metadata": {
        "id": "srIdfK8yJ43R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lines = tf.data.TextLineDataset(titanic_file)\n",
        "counter = tf.data.experimental.Counter()\n",
        "\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "dataset = dataset.shuffle(buffer_size = 100)\n",
        "dataset = dataset.batch(20)\n",
        "dataset"
      ],
      "metadata": {
        "id": "-JqJHsYJJxUJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n, line_batch = next(iter(dataset))\n",
        "print(n.numpy())\n",
        "# buffer_size가 100, batch_size가 20이므로 1번째 배치에 120 이상의 인덱스를 가진 요소가 없다.\n",
        "#  넣고 섞는거니까 100 이상이 없어야 하는 거 아니냐?"
      ],
      "metadata": {
        "id": "-6HsAxJUKN_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`Dataset.shuffle`은 버퍼가 비워질 때까지 에포크의 끝을 알리지 않는다.\n"
      ],
      "metadata": {
        "id": "lIOuGkfTKTLl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "# 1. 셔플 -> repeat\n",
        "shuffled = dataset.shuffle(buffer_size = 100).batch(10).repeat(2)\n",
        "\n",
        "for n, line_batch in shuffled.skip(60).take(5):\n",
        "  print(n.numpy())"
      ],
      "metadata": {
        "id": "ig7LiEFwKRcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "shuffle_repeat = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "plt.plot(shuffle_repeat, label=\"shuffle().repeat()\")\n",
        "plt.ylabel(\"Mean Item ID\")\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "p0lUs6kqKyWe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 반복이 셔플 앞에 있으면 epoch 경계가 섞인다.\n",
        "# 2. repeat -> shuffle\n",
        "dataset = tf.data.Dataset.zip((counter, lines))\n",
        "shuffled = dataset.repeat(2).shuffle(buffer_size = 100).batch(10)\n",
        "\n",
        "for n, line_batch in shuffled.skip(55).take(15):\n",
        "  print(n.numpy())"
      ],
      "metadata": {
        "id": "dtjaKYSUK9c0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "repeat_shuffle = [n.numpy().mean() for n, line_batch in shuffled]\n",
        "\n",
        "plt.plot(shuffle_repeat, label = 'shuffle().repeat()')\n",
        "plt.plot(repeat_shuffle, label=\"repeat().shuffle()\")\n",
        "plt.ylabel('Mean Item ID')\n",
        "plt.legend()"
      ],
      "metadata": {
        "id": "TgKYyGTfLSjD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 데이터 전처리하기\n",
        "- `Dataset.map(f)` 변환은 f를 입력 데이터세트의 각 요소에 적용한다.\n",
        "- `Dataset.map()` 사용법에 대한 일반적인 예를 다룬다."
      ],
      "metadata": {
        "id": "xXv0NyDoLrz1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "list_ds = tf.data.Dataset.list_files(str(flowers_root/\n",
        "                                         '*/*'))"
      ],
      "metadata": {
        "id": "WwILr3RULm0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터세트 요소 조작 함수\n",
        "def parse_image(filename):\n",
        "  parts = tf.strings.split(filename, os.sep)\n",
        "  label = parts[-2]\n",
        "\n",
        "  # 거의 고정된 레파토리임 - 정규화는 하지 않은 것에 주목\n",
        "  image = tf.io.read_file(filename)\n",
        "  image = tf.image.decode_jpeg(image) \n",
        "  image = tf.image.convert_image_dtype(image, tf.float32)\n",
        "  image = tf.image.resize(image, [128, 128])\n",
        "\n",
        "  return image, label"
      ],
      "metadata": {
        "id": "U8TbcN6ML3g_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = next(iter(list_ds))\n",
        "image, label = parse_image(file_path)\n",
        "\n",
        "def show(image, label):\n",
        "  plt.figure()\n",
        "  plt.imshow(image)\n",
        "  plt.title(label.numpy().decode('utf-8'))\n",
        "  plt.axis('off')\n",
        "\n",
        "show(image, label)"
      ],
      "metadata": {
        "id": "auhF7MG5MIpn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 데이터세트에 매핑하기\n",
        "images_ds = list_ds.map(parse_image)\n",
        "\n",
        "for image, label in images_ds.take(2):\n",
        "  show(image, label)"
      ],
      "metadata": {
        "id": "GAPmtXtkMWDl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 임의의 파이썬 로직 적용하기\n",
        "- 가능한 텐서플로우의 작업을 사용하는 걸 권유함.\n",
        "- 그러나 때로는 파이썬 라이브러리를 호출하는 게 유용한 경우가 있다. `Dataset.map()` 변환에서 `tf.py_function()` 연산을 사용할 수 있다.\n",
        "  - 예를 들면 임의의 회전을 적용할 때 `tf.image` 모듈에는 `tf.image.rot90`만 있기 때문에 확대에는 유용하지 않음.\n",
        "  - 참고 : `tensorflow_addons`에는 `tensorflow_addons.image.rotate`에서 호환되는 `rotate`가 있다.\n",
        "  "
      ],
      "metadata": {
        "id": "SXlcLK7iMjhs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import scipy.ndimage as ndimage\n",
        "\n",
        "def random_rotate_image(image):\n",
        "  image = ndimage.rotate(image, np.random.uniform(-30, 30), reshape = False)\n",
        "  return image\n"
      ],
      "metadata": {
        "id": "3mZOs5IcMe1w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image, label = next(iter(images_ds))\n",
        "image = random_rotate_image(image)\n",
        "show(image, label)"
      ],
      "metadata": {
        "id": "2VUOh8A_NAeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 위에서 정의한 함수를 `Dataset.map`과 함께 쓴다면 함수를 적용할 때 반환 형상과 유형도 설명해야 한다.\n"
      ],
      "metadata": {
        "id": "A-MTFaDQNJlu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_random_rotate_image(image, label):\n",
        "  im_shape = image.shape\n",
        "  [image, ] = tf.py_function(random_rotate_image, [image], [tf.float32]) # 설띵충\n",
        "  image.set_shape(im_shape)\n",
        "  return image, label\n",
        "\n",
        "rot_ds = images_ds.map(tf_random_rotate_image)\n",
        "\n",
        "for image,label in rot_ds.take(2):\n",
        "  show(image, label)"
      ],
      "metadata": {
        "id": "AB_YiwgZNEeT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### tf.Example 프로토콜 버퍼 메시지 구문 분석\n",
        "- 많은 입력 파이프라인이 TFRecord 형식에서 `tf.train.Example` 프로토콜 버퍼 메시지를 추출함\n",
        "- 각 `tf.train.Example` 레코드에는 하나 이상의 기능이 포함되며, 입력 파이프라인은 일반적으로 이러한 기능을 텐서로 변환함"
      ],
      "metadata": {
        "id": "Pk4TreJFNtWl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "fsns_test_file = tf.keras.utils.get_file('fsns.tfrec', 'https://storage.googleapis.com/download.tensorflow.org/data/fsns-20160927/testdata/fsns-00000-of-00001')\n"
      ],
      "metadata": {
        "id": "FRlIGAPdNfMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])\n",
        "dataset"
      ],
      "metadata": {
        "id": "Rg9GN_7eOAXZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 데이터 이해를 위해 `tf.data.Dataset` 외부에서 `tf.train.Example` 프로토콜로 작업할 수 있다."
      ],
      "metadata": {
        "id": "0GrfZhzROFm2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "raw_example = next(iter(dataset))\n",
        "parsed = tf.train.Example.FromString(raw_example.numpy())\n",
        "\n",
        "feature = parsed.features.feature\n",
        "raw_img = feature['image/encoded'].bytes_list.value[0]\n",
        "img = tf.image.decode_png(raw_img)\n",
        "plt.imshow(img)\n",
        "plt.axis('off')\n",
        "plt.title(feature['image/text'].bytes_list.value[0])"
      ],
      "metadata": {
        "id": "gd_CeFm-OD6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_example = next(iter(dataset))"
      ],
      "metadata": {
        "id": "YXBozpbtOXNo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_parse(eg):\n",
        "  example = tf.io.parse_example(\n",
        "      eg[tf.newaxis], {\n",
        "          'image/encoded' : tf.io.FixedLenFeature(shape = (), dtype = tf.string),\n",
        "          'image/text' : tf.io.FixedLenFeature(shape = (), dtype = tf.string)\n",
        "      }\n",
        "  )\n",
        "  return example['image/encoded'][0], example['image/text'][0]"
      ],
      "metadata": {
        "id": "i9ens8UKOcwO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "img, txt = tf_parse(raw_example)\n",
        "print(txt.numpy())\n",
        "print(repr(img.numpy()[:20]), \"...\")"
      ],
      "metadata": {
        "id": "y2yiNNRTOvWN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoded = dataset.map(tf_parse)\n",
        "decoded"
      ],
      "metadata": {
        "id": "Ttl1a2q1O0D2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "image_batch, text_batch = next(iter(decoded.batch(10)))\n",
        "image_batch.shape"
      ],
      "metadata": {
        "id": "jHGhnZgOO6p2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 시계열 윈도잉"
      ],
      "metadata": {
        "id": "QyQ3FcUSO-9I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_ds = tf.data.Dataset.range(100000) # 1e6으로 넣었는데 안됨"
      ],
      "metadata": {
        "id": "EfSIeGsSO9sW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 시계열 데이터는 시간에 따라 데이터가 구성되어 있음 : 이를 batch 처리하는 게 제일 간단함\n",
        "batches = range_ds.batch(10, drop_remainder = True)\n",
        "\n",
        "for batch in batches.take(5):\n",
        "  print(batch.numpy())"
      ],
      "metadata": {
        "id": "xwY7rGFbPCad"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 미래 밀집 예측 : 기능, 레이블을 1단계씩 이동시킴\n",
        "def dense_1_step(batch):\n",
        "  return batch[:-1], batch[1:]\n",
        "\n",
        "predict_dense_1_step = batches.map(dense_1_step)\n",
        "\n",
        "for features, label in predict_dense_1_step.take(3):\n",
        "  print(features.numpy(), \" => \", label.numpy())"
      ],
      "metadata": {
        "id": "qPFoYx6CPS2J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 전체 윈도우 예측 : batch를 2개로 분할함\n",
        "batches = range_ds.batch(15, drop_remainder = True)\n",
        "\n",
        "def label_next_5_steps(batch):\n",
        "  return (batch[:-5], batch[-5:]) # Inputs(마지막 5스텝 제외), Labels(마지막 5스텝)\n",
        "\n",
        "predict_5_steps = batches.map(label_next_5_steps)\n",
        "\n",
        "for features, label in predict_5_steps.take(3):\n",
        "  print(features.numpy(), ' => ', label.numpy())"
      ],
      "metadata": {
        "id": "JsjjcEW9PjLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 기능과 레이블 겹치기 : Dataset.zip\n",
        "feature_length = 10\n",
        "label_length = 3\n",
        "\n",
        "features = range_ds.batch(feature_length, drop_remainder = True)\n",
        "labels = range_ds.batch(feature_length).skip(1).map(lambda labels : labels[:label_length])\n",
        "\n",
        "predicted_steps = tf.data.Dataset.zip((features, labels))\n",
        "\n",
        "for features, label in predicted_steps.take(5):\n",
        "  print(features.numpy(), ' -> ', label.numpy())"
      ],
      "metadata": {
        "id": "aFO_67xzP5fB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 보통은 window 쓰죠?\n",
        "window_size = 5\n",
        "windows = range_ds.window(window_size, shift = 1)\n",
        "for sub_ds in windows.take(5):\n",
        "  print(sub_ds)"
      ],
      "metadata": {
        "id": "_auQBVQXQOV1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for x in windows.flat_map(lambda x: x).take(30):\n",
        "  print(x.numpy(), end = ' ')"
      ],
      "metadata": {
        "id": "nSvg559KQVb1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 거의 모든 경우 데이터세트를 batch 처리함\n",
        "def sub_to_batch(sub):\n",
        "  return sub.batch(window_size, drop_remainder=True)\n",
        "\n",
        "for example in windows.flat_map(sub_to_batch).take(5):\n",
        "  print(example.numpy())"
      ],
      "metadata": {
        "id": "PPoF3l_MQaYv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# shift 인수에 의해 윈도우 이동 정도가 제어된다.\n",
        "def make_window_dataset(ds, window_size = 5, shift = 1, stride = 1):\n",
        "  windows = ds.window(window_size, shift = shift, stride = stride)\n",
        "\n",
        "  def sub_to_batch(sub):\n",
        "    return sub.batch(window_size, drop_remainder = True)\n",
        "\n",
        "  windows = windows.flat_map(sub_to_batch)\n",
        "  return windows"
      ],
      "metadata": {
        "id": "zUrst7dFQkUV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = make_window_dataset(range_ds, window_size = 10, shift = 5, stride = 3)\n",
        "\n",
        "for example in ds.take(10):\n",
        "  print(example.numpy())"
      ],
      "metadata": {
        "id": "D21Hye5yQ0rv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 레이블 추출\n",
        "dense_labels_ds = ds.map(dense_1_step)\n",
        "\n",
        "for inputs, labels in dense_labels_ds.take(3):\n",
        "  print(inputs.numpy(), '->', labels.numpy())"
      ],
      "metadata": {
        "id": "u8YhvpwRQ8Mr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 리샘플링\n",
        "- 클래스 불균형이 매우 높은 데이터세트의 경우 데이터 세트를 다시 샘플링해아 함.\n",
        "`tf.Data`는 2가지 방법을 제공한다."
      ],
      "metadata": {
        "id": "yC4oWWviRGvv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "zip_path = tf.keras.utils.get_file(origin = 'https://storage.googleapis.com/download.tensorflow.org/data/creditcard.zip',\n",
        "                                   fname = 'creditcard.zip',\n",
        "                                   extract = True)\n",
        "\n",
        "csv_path = zip_path.replace('.zip', '.csv')"
      ],
      "metadata": {
        "id": "5CelVKXgRDfv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "creditcard_ds = tf.data.experimental.make_csv_dataset(\n",
        "    csv_path, batch_size = 1024, label_name = 'Class',\n",
        "    column_defaults = [float()] * 30 + [int()]\n",
        ")"
      ],
      "metadata": {
        "id": "nlthUISLRWhd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def count(counts, batch):\n",
        "  features, labels = batch\n",
        "  class_1 = labels == 1\n",
        "  class_1 = tf.cast(class_1, tf.int32)\n",
        "\n",
        "  class_0 = labels == 0\n",
        "  class_0 = tf.cast(class_0, tf.int32)\n",
        "\n",
        "  counts['class_0'] += tf.reduce_sum(class_0)\n",
        "  counts['class_1'] += tf.reduce_sum(class_1)\n",
        "\n",
        "  return counts"
      ],
      "metadata": {
        "id": "p8MkckL8RrMX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "counts = creditcard_ds.take(10).reduce(initial_state = {'class_0' : 0, 'class_1' : 0}, reduce_func = count)\n",
        "\n",
        "counts = np.array([counts['class_0'].numpy(), counts['class_1'].numpy()]).astype(np.float32)\n",
        "\n",
        "fractions = counts/counts.sum()\n",
        "\n",
        "print(fractions)"
      ],
      "metadata": {
        "id": "xiC3wRl8R5gY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 데이터 세트 리샘플링하기"
      ],
      "metadata": {
        "id": "UbOuAFENSJ_Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "negative_ds = (creditcard_ds.unbatch().filter(lambda features, label : label == 0).repeat())\n",
        "positive_ds = (creditcard_ds.unbatch().filter(lambda features, label : label == 1).repeat())"
      ],
      "metadata": {
        "id": "O7nYT0qaSGBp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, label in positive_ds.batch(10).take(1):\n",
        "  print(label.numpy())"
      ],
      "metadata": {
        "id": "kR3YmyDASYpQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`tf.data.experimental.sample_from_dataset`를 쓰기 위해 데이터 세트와 각각의 가중치를 전달한다."
      ],
      "metadata": {
        "id": "SkDk9JVgSfIc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_ds = tf.data.experimental.sample_from_datasets([negative_ds, positive_ds], [0.5, 0.5]).batch(10)"
      ],
      "metadata": {
        "id": "4jDkbyFgSbXj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ],
      "metadata": {
        "id": "WBWhErtXSoSi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "`experimental.sample_from_datasets` 방식의 문제 : 개별 `tf.data.Dataset`이 필요하다는 점이다. `Dataset.filter`를 사용하면 되지만 모든 데이터가 2번 로드된다.\n",
        "#### `data.experimental.rejection_resample` 이라는 게 있음\n",
        "- 1번만 로드하면서 균형을 재조정함. \n",
        "- `class_func` 인수가 사용된다. 그냥 예제 보자."
      ],
      "metadata": {
        "id": "qhaUQ7LiSxd_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def class_func(features, label):\n",
        "  return label"
      ],
      "metadata": {
        "id": "Ti2lgJrxSsp4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 이거!\n",
        "resampler = tf.data.experimental.rejection_resample(\n",
        "    class_func, target_dist=[.5, .5], initial_dist = fractions\n",
        ")"
      ],
      "metadata": {
        "id": "Gy7cynz8TGLv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 리샘플러는 개별 예시를 다루므로 unbatch 먼저 한다\n",
        "resample_ds = creditcard_ds.unbatch().apply(resampler).batch(10)"
      ],
      "metadata": {
        "id": "batLKd7ETMb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "balanced_ds = resample_ds.map(lambda extra_label, features_and_label: features_and_label)"
      ],
      "metadata": {
        "id": "rspRGvW7TRkn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for features, labels in balanced_ds.take(10):\n",
        "  print(labels.numpy())"
      ],
      "metadata": {
        "id": "dMrWI5wXTX6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 반복기 검사점 처리 `tf.train.Checkpoint`"
      ],
      "metadata": {
        "id": "pxmXDGmPTo81"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "range_ds = tf.data.Dataset.range(20)\n",
        "\n",
        "iterator = iter(range_ds)\n",
        "ckpt = tf.train.Checkpoint(step = tf.Variable(0), iterator = iterator)\n",
        "manager = tf.train.CheckpointManager(ckpt, '/tmp/my_ckpt', max_to_keep = 3)\n",
        "print([next(iterator).numpy() for _ in range(5)])\n",
        "\n",
        "save_path = manager.save()\n",
        "\n",
        "print([next(iterator).numpy() for _ in range (5)])\n",
        "\n",
        "ckpt.restore(manager.latest_checkpoint)\n",
        "\n",
        "print([next(iterator).numpy() for _ in range(5)])"
      ],
      "metadata": {
        "id": "fZUN2KYkTgRf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `tf.py_function` 같은 외부 상태에 의존하는 반복기는 검사점 처리할 수 없다."
      ],
      "metadata": {
        "id": "ojM5W4HTUL2T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 고급 API 사용하기"
      ],
      "metadata": {
        "id": "s8AxkJ-uUSOk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train, test = tf.keras.datasets.fashion_mnist.load_data()\n",
        "\n",
        "images, labels = train\n",
        "images = images / 255.0\n",
        "labels = labels.astype(np.int32)"
      ],
      "metadata": {
        "id": "2wcTKWYcUJXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fmnist_train_ds = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "fmnist_train_ds = fmnist_train_ds.shuffle(5000).batch(32)\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "                             tf.keras.layers.Flatten(),\n",
        "                             tf.keras.layers.Dense(10)\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits = True),\n",
        "              metrics = ['accuracy'])"
      ],
      "metadata": {
        "id": "JMhtko6UUa7G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(fmnist_train_ds,epochs = 2)"
      ],
      "metadata": {
        "id": "Y9eZl_VeUsY8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- `Dataset.repeat()`를 호출해 무한한 데이터셋을 전달한다면 추가로 `steps_per_epoch`인수를 전달해줘야 한다"
      ],
      "metadata": {
        "id": "SPavTcskUyVu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.fit(fmnist_train_ds.repeat(), epochs = 2, steps_per_epoch = 20)"
      ],
      "metadata": {
        "id": "dAgIGOS3Ut8D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "loss, accuracy = model.evaluate(fmnist_train_ds)\n",
        "print(\"Loss : \", loss)\n",
        "print(\"Accuracy : \", accuracy)"
      ],
      "metadata": {
        "id": "rq7649eDU-Yc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict_ds = tf.data.Dataset.from_tensor_slices(images).batch(32)\n",
        "result = model.predict(predict_ds, steps = 10)\n",
        "print(result.shape)"
      ],
      "metadata": {
        "id": "37xvBZrIVEmg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result= model.predict(fmnist_train_ds, steps = 10)\n",
        "print(result.shape)"
      ],
      "metadata": {
        "id": "ftME0lS4VNXM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1NqoIh7NVRTu"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}