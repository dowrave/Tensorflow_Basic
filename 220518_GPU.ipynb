{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "220518_GPU.ipynb",
      "provenance": [],
      "authorship_tag": "ABX9TyOinuNFWP9AzGjqm7h8rZtL",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dowrave/Tensorflow_Basic/blob/main/220518_GPU.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mepsRCpHHj65"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 할당된 장치 보기\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "a = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "b = tf.constant([[1., 2.], [3., 4.], [5. ,6.]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p03Ng83wHp3n",
        "outputId": "d1def5cf-efc1-44da-85ca-488be6901840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 수동으로 장치 할당하기\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "with tf.device('/CPU:0'):\n",
        "  a = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "  b = tf.constant([[1., 2.], [3., 4.], [5. ,6.]])\n",
        "\n",
        "c = tf.matmul(a, b) # 수행할 장치 명시적인 할당 X - 가용한 장치들 중 하나를 고르고, 필요하다면 텐서를 자동으로 복사함\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vnl-rRxjL9Wp",
        "outputId": "fc907be3-70b2-4150-a584-94ca3ebd3fdf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GPU 메모리 제한\n",
        "`tf.config.experimental.set_visible_devices` : 텐서플로우에서 접근할 수 있는 GPU 조정 가능"
      ],
      "metadata": {
        "id": "wN8m4e7uMW5d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_visible_devices(gpus[0], 'GPU')\n",
        "  except RuntimeError as e:\n",
        "    print(e)"
      ],
      "metadata": {
        "id": "_ktkAfKgMIJg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 가용한 메모리 일부에만 할당되도록 하거나, 프로세스 요구량만큼 메모리 사용이 가능할 필요가 있다.\n",
        "1. `tf.config.experimental.set_memory_growth` : 메모리 증가 허용 - 런타임에서 할당하는데 필요한 양만큼의 gpu 메모리를 할당함."
      ],
      "metadata": {
        "id": "wgFO5A8qMrm7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    tf.config.experimental.set_memory_growth(gpus[0], True)\n",
        "  except RuntimeError as e:\n",
        "    # 에러 : 프로그램 시작 시에 메모리 증가가 설정되어야 함\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p5qi_P8CMolR",
        "outputId": "8a17f272-568c-401e-bb4d-8bf58aa50150"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Physical devices cannot be modified after being initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. `tf.config.experimental.set_virtual_device_configuration`으로 가상 GPU 장치를 설정, GPU에 할당될 전체 메모리를 제한함"
      ],
      "metadata": {
        "id": "pMePVDeoNL0h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "\n",
        "if gpus:\n",
        "  # 1번째 gpu에 1gb 메모리만 할당함\n",
        "  try:\n",
        "    tf.config.experimental.set_virtual_device_configuration(\n",
        "        gpus[0],\n",
        "        [tf.config.experimental.VirtualDeviceConfiguration(memory_limit = 1024)]\n",
        "    )\n",
        "  except RuntimeError as e:\n",
        "    # 얘도 에러는 똑같이 뜸 - 이미 초기화를 했기 때문\n",
        "    print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PmC0mkT2M8k0",
        "outputId": "4b2048ac-c552-4adb-8a49-331de246f94c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Virtual devices cannot be modified after being initialized\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티 GPU 시스템에서 하나의 GPU만 사용하기\n",
        "- 2개 이상의 GPU가 있다면 기본적으로 낮은 GPU가 선택됨. 다른 GPU에서 실행하고 싶다면 명시적인 표시가 필요함"
      ],
      "metadata": {
        "id": "Q58TZ2efNh-T"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "try:\n",
        "  with tf.device('/device:GPU:2'):\n",
        "    a = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "    b = tf.constant([[1., 2.], [3., 4.], [5., 6.]])\n",
        "    c = tf.matmul(a, b)\n",
        "except RuntimeError as e:\n",
        "  print(e)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eU3bq7VUNd-Y",
        "outputId": "329d76e3-3902-4a10-966d-cc7cc94385cc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 명시한 장치가 없을 때 텐서플로우가 자동으로 현재 지원하는 장치를 선택하게 함 : tf.config.set_soft_device_placement(True)\n",
        "tf.config.set_soft_device_placement(True)\n",
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "a = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "b = tf.constant([[1., 2.], [3., 4.], [5., 6.]])\n",
        "c = tf.matmul(a, b)\n",
        "\n",
        "print(c)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxXkIOUDNvWk",
        "outputId": "8055b56b-f7a5-4add-857f-0c531b9fd536"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 멀티 GPU 사용하기 : `tf.distribute.Strategy`\n",
        "- `MirroredStrategy` : 입력 데이터를 나누고 모델의 각 복사본을 각 GPU에서 실행함. `데이터 병렬처리` 라고도 함."
      ],
      "metadata": {
        "id": "KOEfvOOmOFf0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "strategy = tf.distribute.MirroredStrategy()\n",
        "\n",
        "with strategy.scope():\n",
        "  inputs = tf.keras.layers.Input(shape = (1, ))\n",
        "  predictions = tf.keras.layers.Dense(1)(inputs)\n",
        "  model = tf.keras.models.Model(inputs = inputs, outputs = predictions)\n",
        "  model.compile(loss = 'mse',\n",
        "                optimizer = tf.keras.optimizers.SGD(learning_rate = 0.2))"
      ],
      "metadata": {
        "id": "Yt5qmKpTODQn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### `tf.distribute.strategy` 미사용\n",
        "- `tf.distribute.Strategy`는 여러 장치에 걸쳐 계산을 복제해서 동작함. 모델을 각 GPU에 구성해서 수동으로 이를 구현할 수도 있다."
      ],
      "metadata": {
        "id": "Y_3H41iKOmFN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf.debugging.set_log_device_placement(True)\n",
        "\n",
        "gpus = tf.config.experimental.list_logical_devices('GPU')\n",
        "if gpus:\n",
        "  c = []\n",
        "  for gpu in gpus:\n",
        "    with tf.device(gpu.name):\n",
        "      a = tf.constant([[1., 2., 3.], [4., 5., 6.]])\n",
        "      b = tf.constant([[1., 2.], [3., 4.], [5., 6.]])\n",
        "      c.append(tf.matmul(a, b))\n",
        "\n",
        "  with tf.device('/CPU:0'):\n",
        "    matmul_sum = tf.add_n(c)\n",
        "\n",
        "  print(matmul_sum)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s7DtoNUiOXb0",
        "outputId": "fbed8f2a-8709-410b-9556-6b92e2f93aab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op _EagerConst in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "Executing op MatMul in device /job:localhost/replica:0/task:0/device:GPU:0\n",
            "tf.Tensor(\n",
            "[[22. 28.]\n",
            " [49. 64.]], shape=(2, 2), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "2eVFyKpXO9Er"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}